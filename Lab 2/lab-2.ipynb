{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Feedforward Network and Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data generation\n",
    "# Training data\n",
    "X = np.random.randint(2, size=[50, 2])\n",
    "Z = np.array([X[:, 0] ^ X[:, 1]]).T\n",
    "\n",
    "# Test data\n",
    "X_Test = np.random.randint(2, size=[50, 2])\n",
    "Z_Test = np.array([X_Test[:, 0] ^ X_Test[:, 1]]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise weights and biases\n",
    "W1 = np.random.randn(3, 2)\n",
    "B1 = np.random.randn(3)\n",
    "W2 = np.random.randn(1, 3)\n",
    "B2 = np.random.randn(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you explain why we set the dimensions of the weights as (3, 2) and (1, 3)?\n",
    "\n",
    "The weights were set as (3, 2) and (1, 3) because the data now needs to train/test a hidden layer of 3 neurons and one final neuron, as opposed to simply one as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "\n",
    "def sigm(X, W, B):\n",
    "    \"\"\"\n",
    "    Sigmoid Function, a type of activation function\n",
    "    X = inputs\n",
    "    W = weights\n",
    "    B = bias\n",
    "    \"\"\"\n",
    "    M = 1/(1 + np.exp(-(X.dot(W.T) + B)))\n",
    "    return M\n",
    "\n",
    "\n",
    "def Forward(X, W1, B1, W2, B2):\n",
    "\n",
    "    # First Layer\n",
    "    H = sigm(X, W1, B1)\n",
    "\n",
    "    # Second Layer\n",
    "    Y = sigm(H, W2, B2)\n",
    "\n",
    "    # Return the final output and the output from the final layer\n",
    "    return Y, H\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the presence of the hidden layer, we need to use the backpropagation algorithm to update the weights.\n",
    "\n",
    "$$\n",
    "E=(z-y)^2\\\\\n",
    "w_i'=w_i+\\eta\\frac{dE}{dw_i}\\\\\n",
    "b'=b+\\eta\\frac{dE}{db}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update rules for weights and bias\n",
    "\n",
    "def diff_B2(Z, Y):\n",
    "    # Calculate the derivative of B\n",
    "    dB = (Z - Y) * Y * (1 - Y)\n",
    "\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "\n",
    "def diff_W2(H, Z, Y):\n",
    "    dW = (Z - Y) * Y * (1 - Y)\n",
    "    return H.T.dot(dW)\n",
    "\n",
    "\n",
    "def diff_W1(X, H, Z, Y, W2):\n",
    "    dZ = (Z - Y).dot(W2) * Y * (1 - Y) * H * (1 - H)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "\n",
    "def diff_B1(Z, Y, W2, H):\n",
    "    return ((Z - Y).dot(W2) * Y * (1 - Y) * H * (1 - H)).sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.5430394324536787\n",
      "Epoch: 50, Accuracy: 0.7425259099607534\n",
      "Epoch: 100, Accuracy: 0.754459128236867\n",
      "Epoch: 150, Accuracy: 0.758340072994752\n",
      "Epoch: 200, Accuracy: 0.7619051277189562\n",
      "Epoch: 250, Accuracy: 0.7652133977461756\n",
      "Epoch: 300, Accuracy: 0.7683059604791087\n",
      "Epoch: 350, Accuracy: 0.7712490550791076\n",
      "Epoch: 400, Accuracy: 0.7741305803989659\n",
      "Epoch: 450, Accuracy: 0.7770433848372265\n",
      "Epoch: 500, Accuracy: 0.7800694274685428\n",
      "Epoch: 550, Accuracy: 0.7832703517981404\n",
      "Epoch: 600, Accuracy: 0.7866839120605373\n",
      "Epoch: 650, Accuracy: 0.7903242782076466\n",
      "Epoch: 700, Accuracy: 0.7941848748606266\n",
      "Epoch: 750, Accuracy: 0.7982430831308162\n",
      "Epoch: 800, Accuracy: 0.8024663282391316\n",
      "Epoch: 850, Accuracy: 0.8068189671687829\n",
      "Epoch: 900, Accuracy: 0.811269271774087\n",
      "Epoch: 950, Accuracy: 0.8157958385658917\n",
      "Epoch: 1000, Accuracy: 0.8203929258284444\n",
      "Epoch: 1050, Accuracy: 0.8250743531226459\n",
      "Epoch: 1100, Accuracy: 0.8298754800893309\n",
      "Epoch: 1150, Accuracy: 0.8348523121700382\n",
      "Epoch: 1200, Accuracy: 0.8400762711681488\n",
      "Epoch: 1250, Accuracy: 0.8456236462286049\n",
      "Epoch: 1300, Accuracy: 0.8515614560842387\n",
      "Epoch: 1350, Accuracy: 0.8579354465970146\n",
      "Epoch: 1400, Accuracy: 0.8647661284692276\n",
      "Epoch: 1450, Accuracy: 0.8720515546111087\n",
      "Epoch: 1500, Accuracy: 0.8797678349101355\n",
      "Epoch: 1550, Accuracy: 0.8878606336135277\n",
      "Epoch: 1600, Accuracy: 0.8962314066587356\n",
      "Epoch: 1650, Accuracy: 0.9047302834380027\n",
      "Epoch: 1700, Accuracy: 0.9131663593740537\n",
      "Epoch: 1750, Accuracy: 0.921335873012295\n",
      "Epoch: 1800, Accuracy: 0.9290567253527839\n",
      "Epoch: 1850, Accuracy: 0.9361944627976728\n",
      "Epoch: 1900, Accuracy: 0.942672109837974\n",
      "Epoch: 1950, Accuracy: 0.9484657912963848\n",
      "Epoch: 2000, Accuracy: 0.9535926209537231\n",
      "Epoch: 2050, Accuracy: 0.9580967597970856\n",
      "Epoch: 2100, Accuracy: 0.9620371139971252\n",
      "Epoch: 2150, Accuracy: 0.9654780327272383\n",
      "Epoch: 2200, Accuracy: 0.96848309574471\n",
      "Epoch: 2250, Accuracy: 0.9711114781585195\n",
      "Epoch: 2300, Accuracy: 0.9734161962781285\n",
      "Epoch: 2350, Accuracy: 0.9754435839491595\n",
      "Epoch: 2400, Accuracy: 0.9772334905228652\n",
      "Epoch: 2450, Accuracy: 0.9788198474780112\n",
      "Epoch: 2500, Accuracy: 0.980231381282472\n",
      "Epoch: 2550, Accuracy: 0.9814923445171017\n",
      "Epoch: 2600, Accuracy: 0.9826231991274271\n",
      "Epoch: 2650, Accuracy: 0.983641223122981\n",
      "Epoch: 2700, Accuracy: 0.984561033129228\n",
      "Epoch: 2750, Accuracy: 0.9853950260909821\n",
      "Epoch: 2800, Accuracy: 0.9861537483465869\n",
      "Epoch: 2850, Accuracy: 0.9868462019085914\n",
      "Epoch: 2900, Accuracy: 0.9874800976991558\n",
      "Epoch: 2950, Accuracy: 0.9880620646128844\n",
      "Epoch: 3000, Accuracy: 0.9885978221165947\n",
      "Epoch: 3050, Accuracy: 0.9890923229026268\n",
      "Epoch: 3100, Accuracy: 0.9895498710109443\n",
      "Epoch: 3150, Accuracy: 0.9899742198726813\n",
      "Epoch: 3200, Accuracy: 0.9903686539129065\n",
      "Epoch: 3250, Accuracy: 0.990736056673967\n",
      "Epoch: 3300, Accuracy: 0.991078967866203\n",
      "Epoch: 3350, Accuracy: 0.9913996313015977\n",
      "Epoch: 3400, Accuracy: 0.991700035300477\n",
      "Epoch: 3450, Accuracy: 0.991981946866086\n",
      "Epoch: 3500, Accuracy: 0.9922469406835005\n",
      "Epoch: 3550, Accuracy: 0.9924964238068518\n",
      "Epoch: 3600, Accuracy: 0.9927316567432806\n",
      "Epoch: 3650, Accuracy: 0.992953771516093\n",
      "Epoch: 3700, Accuracy: 0.9931637871874366\n",
      "Epoch: 3750, Accuracy: 0.9933626232377559\n",
      "Epoch: 3800, Accuracy: 0.9935511111315833\n",
      "Epoch: 3850, Accuracy: 0.9937300043438888\n",
      "Epoch: 3900, Accuracy: 0.9938999870758601\n",
      "Epoch: 3950, Accuracy: 0.9940616818517092\n",
      "Epoch: 4000, Accuracy: 0.99421565615737\n",
      "Epoch: 4050, Accuracy: 0.9943624282565517\n",
      "Epoch: 4100, Accuracy: 0.9945024722985449\n",
      "Epoch: 4150, Accuracy: 0.9946362228146637\n",
      "Epoch: 4200, Accuracy: 0.9947640786855988\n",
      "Epoch: 4250, Accuracy: 0.9948864066497441\n",
      "Epoch: 4300, Accuracy: 0.9950035444123114\n",
      "Epoch: 4350, Accuracy: 0.9951158034064344\n",
      "Epoch: 4400, Accuracy: 0.9952234712502037\n",
      "Epoch: 4450, Accuracy: 0.9953268139374283\n",
      "Epoch: 4500, Accuracy: 0.9954260777947176\n",
      "Epoch: 4550, Accuracy: 0.9955214912330581\n",
      "Epoch: 4600, Accuracy: 0.9956132663182862\n",
      "Epoch: 4650, Accuracy: 0.9957016001816484\n",
      "Epoch: 4700, Accuracy: 0.9957866762888838\n",
      "Epoch: 4750, Accuracy: 0.9958686655839027\n",
      "Epoch: 4800, Accuracy: 0.995947727521107\n",
      "Epoch: 4850, Accuracy: 0.9960240109986472\n",
      "Epoch: 4900, Accuracy: 0.9960976552033991\n",
      "Epoch: 4950, Accuracy: 0.99616879037714\n",
      "Epoch: 5000, Accuracy: 0.996237538512268\n",
      "Epoch: 5050, Accuracy: 0.9963040139844231\n",
      "Epoch: 5100, Accuracy: 0.9963683241285158\n",
      "Epoch: 5150, Accuracy: 0.9964305697639158\n",
      "Epoch: 5200, Accuracy: 0.996490845673906\n",
      "Epoch: 5250, Accuracy: 0.9965492410439326\n",
      "Epoch: 5300, Accuracy: 0.996605839862679\n",
      "Epoch: 5350, Accuracy: 0.9966607212895575\n",
      "Epoch: 5400, Accuracy: 0.9967139599918172\n",
      "Epoch: 5450, Accuracy: 0.9967656264541336\n",
      "Epoch: 5500, Accuracy: 0.9968157872632363\n",
      "Epoch: 5550, Accuracy: 0.9968645053698718\n",
      "Epoch: 5600, Accuracy: 0.9969118403301575\n",
      "Epoch: 5650, Accuracy: 0.9969578485281767\n",
      "Epoch: 5700, Accuracy: 0.997002583381479\n",
      "Epoch: 5750, Accuracy: 0.9970460955309813\n",
      "Epoch: 5800, Accuracy: 0.9970884330166259\n",
      "Epoch: 5850, Accuracy: 0.9971296414400092\n",
      "Epoch: 5900, Accuracy: 0.9971697641150912\n",
      "Epoch: 5950, Accuracy: 0.9972088422079783\n",
      "Epoch: 6000, Accuracy: 0.9972469148666884\n",
      "Epoch: 6050, Accuracy: 0.9972840193417155\n",
      "Epoch: 6100, Accuracy: 0.9973201910981428\n",
      "Epoch: 6150, Accuracy: 0.9973554639199794\n",
      "Epoch: 6200, Accuracy: 0.9973898700073374\n",
      "Epoch: 6250, Accuracy: 0.9974234400670132\n",
      "Epoch: 6300, Accuracy: 0.9974562033969833\n",
      "Epoch: 6350, Accuracy: 0.9974881879652829\n",
      "Epoch: 6400, Accuracy: 0.9975194204836957\n",
      "Epoch: 6450, Accuracy: 0.997549926476644\n",
      "Epoch: 6500, Accuracy: 0.9975797303456381\n",
      "Epoch: 6550, Accuracy: 0.9976088554296133\n",
      "Epoch: 6600, Accuracy: 0.9976373240614529\n",
      "Epoch: 6650, Accuracy: 0.9976651576209757\n",
      "Epoch: 6700, Accuracy: 0.9976923765846405\n",
      "Epoch: 6750, Accuracy: 0.9977190005721998\n",
      "Epoch: 6800, Accuracy: 0.997745048390519\n",
      "Epoch: 6850, Accuracy: 0.9977705380747572\n",
      "Epoch: 6900, Accuracy: 0.9977954869270921\n",
      "Epoch: 6950, Accuracy: 0.9978199115531571\n",
      "Epoch: 7000, Accuracy: 0.9978438278963453\n",
      "Epoch: 7050, Accuracy: 0.9978672512701243\n",
      "Epoch: 7100, Accuracy: 0.9978901963884932\n",
      "Epoch: 7150, Accuracy: 0.9979126773947046\n",
      "Epoch: 7200, Accuracy: 0.9979347078883655\n",
      "Epoch: 7250, Accuracy: 0.997956300951022\n",
      "Epoch: 7300, Accuracy: 0.9979774691703241\n",
      "Epoch: 7350, Accuracy: 0.997998224662863\n",
      "Epoch: 7400, Accuracy: 0.9980185790957625\n",
      "Epoch: 7450, Accuracy: 0.998038543707104\n",
      "Epoch: 7500, Accuracy: 0.9980581293252567\n",
      "Epoch: 7550, Accuracy: 0.9980773463871818\n",
      "Epoch: 7600, Accuracy: 0.9980962049557704\n",
      "Epoch: 7650, Accuracy: 0.9981147147362768\n",
      "Epoch: 7700, Accuracy: 0.9981328850918992\n",
      "Epoch: 7750, Accuracy: 0.9981507250585607\n",
      "Epoch: 7800, Accuracy: 0.9981682433589352\n",
      "Epoch: 7850, Accuracy: 0.998185448415764\n",
      "Epoch: 7900, Accuracy: 0.9982023483645056\n",
      "Epoch: 7950, Accuracy: 0.9982189510653546\n",
      "Epoch: 8000, Accuracy: 0.9982352641146666\n",
      "Epoch: 8050, Accuracy: 0.998251294855824\n",
      "Epoch: 8100, Accuracy: 0.9982670503895734\n",
      "Epoch: 8150, Accuracy: 0.9982825375838638\n",
      "Epoch: 8200, Accuracy: 0.9982977630832134\n",
      "Epoch: 8250, Accuracy: 0.9983127333176319\n",
      "Epoch: 8300, Accuracy: 0.9983274545111216\n",
      "Epoch: 8350, Accuracy: 0.9983419326897799\n",
      "Epoch: 8400, Accuracy: 0.9983561736895267\n",
      "Epoch: 8450, Accuracy: 0.9983701831634734\n",
      "Epoch: 8500, Accuracy: 0.9983839665889562\n",
      "Epoch: 8550, Accuracy: 0.9983975292742484\n",
      "Epoch: 8600, Accuracy: 0.9984108763649703\n",
      "Epoch: 8650, Accuracy: 0.9984240128502114\n",
      "Epoch: 8700, Accuracy: 0.998436943568381\n",
      "Epoch: 8750, Accuracy: 0.9984496732127991\n",
      "Epoch: 8800, Accuracy: 0.9984622063370432\n",
      "Epoch: 8850, Accuracy: 0.998474547360061\n",
      "Epoch: 8900, Accuracy: 0.9984867005710629\n",
      "Epoch: 8950, Accuracy: 0.9984986701342031\n",
      "Epoch: 9000, Accuracy: 0.9985104600930624\n",
      "Epoch: 9050, Accuracy: 0.9985220743749396\n",
      "Epoch: 9100, Accuracy: 0.9985335167949623\n",
      "Epoch: 9150, Accuracy: 0.9985447910600272\n",
      "Epoch: 9200, Accuracy: 0.9985559007725747\n",
      "Epoch: 9250, Accuracy: 0.9985668494342093\n",
      "Epoch: 9300, Accuracy: 0.9985776404491702\n",
      "Epoch: 9350, Accuracy: 0.9985882771276616\n",
      "Epoch: 9400, Accuracy: 0.998598762689048\n",
      "Epoch: 9450, Accuracy: 0.9986091002649204\n",
      "Epoch: 9500, Accuracy: 0.9986192929020403\n",
      "Epoch: 9550, Accuracy: 0.9986293435651672\n",
      "Epoch: 9600, Accuracy: 0.9986392551397741\n",
      "Epoch: 9650, Accuracy: 0.9986490304346566\n",
      "Epoch: 9700, Accuracy: 0.9986586721844404\n",
      "Epoch: 9750, Accuracy: 0.9986681830519916\n",
      "Epoch: 9800, Accuracy: 0.9986775656307345\n",
      "Epoch: 9850, Accuracy: 0.9986868224468808\n",
      "Epoch: 9900, Accuracy: 0.9986959559615732\n",
      "Epoch: 9950, Accuracy: 0.9987049685729491\n"
     ]
    }
   ],
   "source": [
    "# Learning process\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for epoch in range(10000):\n",
    "    Y, H = Forward(X, W1, B1, W2, B2)\n",
    "\n",
    "    W2 += learning_rate * diff_W2(H, Z, Y).T\n",
    "    B2 += learning_rate * diff_B2(Z, Y)\n",
    "    W1 += learning_rate * diff_W1(X, H, Z, Y, W2).T\n",
    "    B1 += learning_rate * diff_B1(Z, Y, W2, H)\n",
    "    if not epoch % 50:\n",
    "        Accuracy = 1 - np.mean((Z - Y)**2)\n",
    "        print(f\"Epoch: {epoch}, Accuracy: {Accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.9987279548265086\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "X_Test = np.random.randint(2, size=[50, 2])\n",
    "Z_Test = np.array([X_Test[:, 0] ^ X_Test[:, 1]]).T\n",
    "Y_Test, H = Forward(X_Test, W1, B1, W2, B2)\n",
    "Accuracy = 1 - np.mean((Z_Test - Y_Test)**2)\n",
    "\n",
    "print('Testing Accuracy: ', Accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20dcf72490f89c174da2c842e68573857612f3c1b1722e617d46dde610562cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
