{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Feedforward Network and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialise weights and biases\n",
    "W1 = np.random.randn(3, 2)\n",
    "B1 = np.random.randn(3)\n",
    "W2 = np.random.randn(1, 3)\n",
    "B2 = np.random.randn(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you explain why we set the dimensions of the weights as (3, 2) and (1, 3)?\n",
    "\n",
    "The weights were set as (3, 2) and (1, 3) because the data now needs to train/test a hidden layer of 3 neurons and one final neuron, as opposed to simply one as before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "\n",
    "def sigm(X, W, B):\n",
    "    \"\"\"\n",
    "    Sigmoid Function, a type of activation function\n",
    "    X = inputs\n",
    "    W = weights\n",
    "    B = bias\n",
    "    \"\"\"\n",
    "    M = 1/(1 + np.exp(-(X.dot(W.T) + B)))\n",
    "    return M\n",
    "\n",
    "def Forward(X, W1, B1, W2, B2):\n",
    "\n",
    "    # First Layer\n",
    "    H = sigm(X, W1, B1)\n",
    "    \n",
    "    # Second Layer\n",
    "    Y = sigm(H, W2, B2)\n",
    "    \n",
    "    # Return the final output and the output from the final layer\n",
    "    return Y, H"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the presence of the hidden layer, we need to use the backpropagation algorithm to update the weights. \n",
    "\n",
    "$$\n",
    "E=(z-y)^2\\\\\n",
    "w_i'=w_i+\\eta\\frac{dE}{dw_i}\\\\\n",
    "b'=b+\\eta\\frac{dE}{db}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update rules for weights and bias\n",
    "\n",
    "def diff_B2(Z, Y):\n",
    "    # Calculate the derivative of B\n",
    "    dB = (Z - Y) * Y * (1 - Y)\n",
    "\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "def diff_W2(H, Z, Y):\n",
    "    dW = (Z - Y) * Y * (1 - Y)\n",
    "    return H.T.dot(dW)\n",
    "\n",
    "def diff_W1(X, H, Z, Y, W2):\n",
    "    dZ = (Z - Y).dot(W2) * Y * (1 - Y) * H * (1 - H)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def diff_B1(Z, Y, W2, H):\n",
    "    return ((Z - Y).got(W2) * Y * (1 - Y) * H * (1 - H)).sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data generation\n",
    "# Training data\n",
    "X = np.random.randint(2, size=[15, 2])\n",
    "Y = np.array([X[:, 0] | X[:, 1]]).T\n",
    "\n",
    "# Test data\n",
    "X_test = np.random.randint(2, size=[15, 2])\n",
    "Y_test = np.array([X_test[:, 0] | X_test[:, 1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m):\n\u001b[0;32m      5\u001b[0m     Y, H \u001b[39m=\u001b[39m Forward(X, W1, B1, W2, B2)\n\u001b[1;32m----> 7\u001b[0m     W2 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m diff_W2(H, Z, Y)\u001b[39m.\u001b[39mT\n\u001b[0;32m      8\u001b[0m     B2 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m diff_B2(Z, Y)\n\u001b[0;32m      9\u001b[0m     W1 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m diff_W1(X, H, Z, Y, W2)\u001b[39m.\u001b[39mT\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "# Learning process\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for epoch in range(10000):\n",
    "    Y, H = Forward(X, W1, B1, W2, B2)\n",
    "    \n",
    "    W2 += learning_rate * diff_W2(H, Z, Y).T\n",
    "    B2 += learning_rate * diff_B2(Z, Y)\n",
    "    W1 += learning_rate * diff_W1(X, H, Z, Y, W2).T\n",
    "    B1 += learning_rate * diff_B1(Z, Y, W2, H)\n",
    "    if not epoch % 50:\n",
    "        Accuracy = 1 - np.mean((Z - Y)**2)\n",
    "        print(f\"Epoch: {epoch}, Accuracy: {Accuracy}\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20dcf72490f89c174da2c842e68573857612f3c1b1722e617d46dde610562cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
