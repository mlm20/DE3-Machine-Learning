{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Feedforward Network and Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data generation\n",
    "# Training data\n",
    "X = np.random.randint(2, size=[50, 2])\n",
    "Z = np.array([X[:, 0] ^ X[:, 1]]).T\n",
    "\n",
    "# Test data\n",
    "X_Test = np.random.randint(2, size=[50, 2])\n",
    "Z_Test = np.array([X_Test[:, 0] ^ X_Test[:, 1]]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise weights and biases\n",
    "W1 = np.random.randn(3, 2)\n",
    "B1 = np.random.randn(3)\n",
    "W2 = np.random.randn(1, 3)\n",
    "B2 = np.random.randn(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you explain why we set the dimensions of the weights as (3, 2) and (1, 3)?\n",
    "\n",
    "The weights were set as (3, 2) and (1, 3) because the data now needs to train/test a hidden layer of 3 neurons and one final neuron, as opposed to simply one as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "\n",
    "def sigm(X, W, B):\n",
    "    \"\"\"\n",
    "    Sigmoid Function, a type of activation function\n",
    "    X = inputs\n",
    "    W = weights\n",
    "    B = bias\n",
    "    \"\"\"\n",
    "    M = 1/(1 + np.exp(-(X.dot(W.T) + B)))\n",
    "    return M\n",
    "\n",
    "\n",
    "def Forward(X, W1, B1, W2, B2):\n",
    "\n",
    "    # First Layer\n",
    "    H = sigm(X, W1, B1)\n",
    "\n",
    "    # Second Layer\n",
    "    Y = sigm(H, W2, B2)\n",
    "\n",
    "    # Return the final output and the output from the final layer\n",
    "    return Y, H\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the presence of the hidden layer, we need to use the backpropagation algorithm to update the weights.\n",
    "\n",
    "$$\n",
    "E=(z-y)^2\\\\\n",
    "w_i'=w_i+\\eta\\frac{dE}{dw_i}\\\\\n",
    "b'=b+\\eta\\frac{dE}{db}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update rules for weights and bias\n",
    "\n",
    "def diff_B2(Z, Y):\n",
    "    # Calculate the derivative of B\n",
    "    dB = (Z - Y) * Y * (1 - Y)\n",
    "\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "\n",
    "def diff_W2(H, Z, Y):\n",
    "    dW = (Z - Y) * Y * (1 - Y)\n",
    "    return H.T.dot(dW)\n",
    "\n",
    "\n",
    "def diff_W1(X, H, Z, Y, W2):\n",
    "    dZ = (Z - Y).dot(W2) * Y * (1 - Y) * H * (1 - H)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "\n",
    "def diff_B1(Z, Y, W2, H):\n",
    "    return ((Z - Y).dot(W2) * Y * (1 - Y) * H * (1 - H)).sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.7453621962499204\n",
      "Epoch: 50, Accuracy: 0.7593693733057082\n",
      "Epoch: 100, Accuracy: 0.7600999928499466\n",
      "Epoch: 150, Accuracy: 0.7609385741450141\n",
      "Epoch: 200, Accuracy: 0.7619183488429944\n",
      "Epoch: 250, Accuracy: 0.7630679960126644\n",
      "Epoch: 300, Accuracy: 0.7644158327030673\n",
      "Epoch: 350, Accuracy: 0.7659879687163238\n",
      "Epoch: 400, Accuracy: 0.7678063720551417\n",
      "Epoch: 450, Accuracy: 0.7698874227757222\n",
      "Epoch: 500, Accuracy: 0.7722414387715153\n",
      "Epoch: 550, Accuracy: 0.7748731807624996\n",
      "Epoch: 600, Accuracy: 0.7777827258767787\n",
      "Epoch: 650, Accuracy: 0.7809658178837792\n",
      "Epoch: 700, Accuracy: 0.7844131199677382\n",
      "Epoch: 750, Accuracy: 0.7881084976905772\n",
      "Epoch: 800, Accuracy: 0.7920270880832452\n",
      "Epoch: 850, Accuracy: 0.7961341466756121\n",
      "Epoch: 900, Accuracy: 0.800385452889443\n",
      "Epoch: 950, Accuracy: 0.8047295257776238\n",
      "Epoch: 1000, Accuracy: 0.8091112854591634\n",
      "Epoch: 1050, Accuracy: 0.8134763461080895\n",
      "Epoch: 1100, Accuracy: 0.8177750181405028\n",
      "Epoch: 1150, Accuracy: 0.8219653288211776\n",
      "Epoch: 1200, Accuracy: 0.8260147779925668\n",
      "Epoch: 1250, Accuracy: 0.8299009223620994\n",
      "Epoch: 1300, Accuracy: 0.8336111006534905\n",
      "Epoch: 1350, Accuracy: 0.8371416626794687\n",
      "Epoch: 1400, Accuracy: 0.8404970112370779\n",
      "Epoch: 1450, Accuracy: 0.8436886796319945\n",
      "Epoch: 1500, Accuracy: 0.8467345949956324\n",
      "Epoch: 1550, Accuracy: 0.8496586318714702\n",
      "Epoch: 1600, Accuracy: 0.8524905354160588\n",
      "Epoch: 1650, Accuracy: 0.8552662740444386\n",
      "Epoch: 1700, Accuracy: 0.8580288471931363\n",
      "Epoch: 1750, Accuracy: 0.8608294947399083\n",
      "Epoch: 1800, Accuracy: 0.8637290816869125\n",
      "Epoch: 1850, Accuracy: 0.866799096495323\n",
      "Epoch: 1900, Accuracy: 0.8701211501590054\n",
      "Epoch: 1950, Accuracy: 0.8737831820176024\n",
      "Epoch: 2000, Accuracy: 0.8778702458876577\n",
      "Epoch: 2050, Accuracy: 0.8824488107360063\n",
      "Epoch: 2100, Accuracy: 0.8875469344191477\n",
      "Epoch: 2150, Accuracy: 0.8931373555827824\n",
      "Epoch: 2200, Accuracy: 0.8991322246070567\n",
      "Epoch: 2250, Accuracy: 0.9053935129684703\n",
      "Epoch: 2300, Accuracy: 0.911755403247808\n",
      "Epoch: 2350, Accuracy: 0.9180506323887926\n",
      "Epoch: 2400, Accuracy: 0.9241332458139235\n",
      "Epoch: 2450, Accuracy: 0.9298929377138027\n",
      "Epoch: 2500, Accuracy: 0.9352595573259334\n",
      "Epoch: 2550, Accuracy: 0.9401995780305739\n",
      "Epoch: 2600, Accuracy: 0.9447080914408293\n",
      "Epoch: 2650, Accuracy: 0.9487997550718534\n",
      "Epoch: 2700, Accuracy: 0.952500872255131\n",
      "Epoch: 2750, Accuracy: 0.9558434538765185\n",
      "Epoch: 2800, Accuracy: 0.9588612404977075\n",
      "Epoch: 2850, Accuracy: 0.9615872841744298\n",
      "Epoch: 2900, Accuracy: 0.9640526252136487\n",
      "Epoch: 2950, Accuracy: 0.9662856747259592\n",
      "Epoch: 3000, Accuracy: 0.9683120224854024\n",
      "Epoch: 3050, Accuracy: 0.9701544849419375\n",
      "Epoch: 3100, Accuracy: 0.9718332780897981\n",
      "Epoch: 3150, Accuracy: 0.9733662464729675\n",
      "Epoch: 3200, Accuracy: 0.9747691089640497\n",
      "Epoch: 3250, Accuracy: 0.9760556997943945\n",
      "Epoch: 3300, Accuracy: 0.9772381938801914\n",
      "Epoch: 3350, Accuracy: 0.9783273116087012\n",
      "Epoch: 3400, Accuracy: 0.9793325017026223\n",
      "Epoch: 3450, Accuracy: 0.9802621026571089\n",
      "Epoch: 3500, Accuracy: 0.9811234841970413\n",
      "Epoch: 3550, Accuracy: 0.9819231706214286\n",
      "Epoch: 3600, Accuracy: 0.9826669480186722\n",
      "Epoch: 3650, Accuracy: 0.9833599572878161\n",
      "Epoch: 3700, Accuracy: 0.9840067747676213\n",
      "Epoch: 3750, Accuracy: 0.9846114821044814\n",
      "Epoch: 3800, Accuracy: 0.9851777268088744\n",
      "Epoch: 3850, Accuracy: 0.9857087747731859\n",
      "Epoch: 3900, Accuracy: 0.986207555859086\n",
      "Epoch: 3950, Accuracy: 0.9866767035137005\n",
      "Epoch: 4000, Accuracy: 0.987118589241565\n",
      "Epoch: 4050, Accuracy: 0.987535352643437\n",
      "Epoch: 4100, Accuracy: 0.9879289276323141\n",
      "Epoch: 4150, Accuracy: 0.9883010653500607\n",
      "Epoch: 4200, Accuracy: 0.988653354233285\n",
      "Epoch: 4250, Accuracy: 0.9889872376130637\n",
      "Epoch: 4300, Accuracy: 0.9893040291783166\n",
      "Epoch: 4350, Accuracy: 0.989604926585849\n",
      "Epoch: 4400, Accuracy: 0.9898910234601367\n",
      "Epoch: 4450, Accuracy: 0.9901633199918415\n",
      "Epoch: 4500, Accuracy: 0.9904227323149579\n",
      "Epoch: 4550, Accuracy: 0.9906701008176474\n",
      "Epoch: 4600, Accuracy: 0.9909061975205912\n",
      "Epoch: 4650, Accuracy: 0.9911317326385372\n",
      "Epoch: 4700, Accuracy: 0.9913473604251779\n",
      "Epoch: 4750, Accuracy: 0.9915536843881726\n",
      "Epoch: 4800, Accuracy: 0.9917512619496963\n",
      "Epoch: 4850, Accuracy: 0.9919406086180783\n",
      "Epoch: 4900, Accuracy: 0.9921222017276345\n",
      "Epoch: 4950, Accuracy: 0.9922964837965219\n",
      "Epoch: 5000, Accuracy: 0.9924638655461511\n",
      "Epoch: 5050, Accuracy: 0.9926247286202657\n",
      "Epoch: 5100, Accuracy: 0.9927794280370936\n",
      "Epoch: 5150, Accuracy: 0.9929282944038966\n",
      "Epoch: 5200, Accuracy: 0.993071635919713\n",
      "Epoch: 5250, Accuracy: 0.9932097401890029\n",
      "Epoch: 5300, Accuracy: 0.9933428758662319\n",
      "Epoch: 5350, Accuracy: 0.9934712941490905\n",
      "Epoch: 5400, Accuracy: 0.9935952301360034\n",
      "Epoch: 5450, Accuracy: 0.9937149040617996\n",
      "Epoch: 5500, Accuracy: 0.9938305224238506\n",
      "Epoch: 5550, Accuracy: 0.9939422790096051\n",
      "Epoch: 5600, Accuracy: 0.9940503558352538\n",
      "Epoch: 5650, Accuracy: 0.9941549240041848\n",
      "Epoch: 5700, Accuracy: 0.9942561444929645\n",
      "Epoch: 5750, Accuracy: 0.9943541688717473\n",
      "Epoch: 5800, Accuracy: 0.9944491399652947\n",
      "Epoch: 5850, Accuracy: 0.9945411924601321\n",
      "Epoch: 5900, Accuracy: 0.9946304534628075\n",
      "Epoch: 5950, Accuracy: 0.9947170430137048\n",
      "Epoch: 6000, Accuracy: 0.9948010745604143\n",
      "Epoch: 6050, Accuracy: 0.9948826553942661\n",
      "Epoch: 6100, Accuracy: 0.9949618870532708\n",
      "Epoch: 6150, Accuracy: 0.9950388656943963\n",
      "Epoch: 6200, Accuracy: 0.9951136824378273\n",
      "Epoch: 6250, Accuracy: 0.9951864236855963\n",
      "Epoch: 6300, Accuracy: 0.9952571714167513\n",
      "Epoch: 6350, Accuracy: 0.9953260034610195\n",
      "Epoch: 6400, Accuracy: 0.9953929937527475\n",
      "Epoch: 6450, Accuracy: 0.9954582125667297\n",
      "Epoch: 6500, Accuracy: 0.9955217267373936\n",
      "Epoch: 6550, Accuracy: 0.9955835998626765\n",
      "Epoch: 6600, Accuracy: 0.9956438924938084\n",
      "Epoch: 6650, Accuracy: 0.9957026623121087\n",
      "Epoch: 6700, Accuracy: 0.995759964293809\n",
      "Epoch: 6750, Accuracy: 0.9958158508638216\n",
      "Epoch: 6800, Accuracy: 0.9958703720393004\n",
      "Epoch: 6850, Accuracy: 0.9959235755637633\n",
      "Epoch: 6900, Accuracy: 0.9959755070324846\n",
      "Epoch: 6950, Accuracy: 0.9960262100098022\n",
      "Epoch: 7000, Accuracy: 0.996075726138936\n",
      "Epoch: 7050, Accuracy: 0.9961240952448608\n",
      "Epoch: 7100, Accuracy: 0.996171355430734\n",
      "Epoch: 7150, Accuracy: 0.9962175431683405\n",
      "Epoch: 7200, Accuracy: 0.9962626933829754\n",
      "Epoch: 7250, Accuracy: 0.9963068395331566\n",
      "Epoch: 7300, Accuracy: 0.9963500136855264\n",
      "Epoch: 7350, Accuracy: 0.9963922465852725\n",
      "Epoch: 7400, Accuracy: 0.9964335677223746\n",
      "Epoch: 7450, Accuracy: 0.9964740053939599\n",
      "Epoch: 7500, Accuracy: 0.9965135867630278\n",
      "Epoch: 7550, Accuracy: 0.9965523379137848\n",
      "Epoch: 7600, Accuracy: 0.9965902839038148\n",
      "Epoch: 7650, Accuracy: 0.9966274488132886\n",
      "Epoch: 7700, Accuracy: 0.9966638557914086\n",
      "Epoch: 7750, Accuracy: 0.9966995271002627\n",
      "Epoch: 7800, Accuracy: 0.9967344841562539\n",
      "Epoch: 7850, Accuracy: 0.9967687475692603\n",
      "Epoch: 7900, Accuracy: 0.9968023371796649\n",
      "Epoch: 7950, Accuracy: 0.9968352720933897\n",
      "Epoch: 8000, Accuracy: 0.996867570715057\n",
      "Epoch: 8050, Accuracy: 0.9968992507793915\n",
      "Epoch: 8100, Accuracy: 0.9969303293809705\n",
      "Epoch: 8150, Accuracy: 0.9969608230024225\n",
      "Epoch: 8200, Accuracy: 0.9969907475411653\n",
      "Epoch: 8250, Accuracy: 0.9970201183347708\n",
      "Epoch: 8300, Accuracy: 0.9970489501850385\n",
      "Epoch: 8350, Accuracy: 0.9970772573808503\n",
      "Epoch: 8400, Accuracy: 0.9971050537198808\n",
      "Epoch: 8450, Accuracy: 0.9971323525292247\n",
      "Epoch: 8500, Accuracy: 0.9971591666850073\n",
      "Epoch: 8550, Accuracy: 0.9971855086310318\n",
      "Epoch: 8600, Accuracy: 0.9972113903965212\n",
      "Epoch: 8650, Accuracy: 0.9972368236130014\n",
      "Epoch: 8700, Accuracy: 0.9972618195303764\n",
      "Epoch: 8750, Accuracy: 0.997286389032239\n",
      "Epoch: 8800, Accuracy: 0.9973105426504564\n",
      "Epoch: 8850, Accuracy: 0.9973342905790743\n",
      "Epoch: 8900, Accuracy: 0.9973576426875723\n",
      "Epoch: 8950, Accuracy: 0.9973806085335072\n",
      "Epoch: 9000, Accuracy: 0.9974031973745757\n",
      "Epoch: 9050, Accuracy: 0.9974254181801289\n",
      "Epoch: 9100, Accuracy: 0.9974472796421635\n",
      "Epoch: 9150, Accuracy: 0.9974687901858225\n",
      "Epoch: 9200, Accuracy: 0.997489957979424\n",
      "Epoch: 9250, Accuracy: 0.9975107909440483\n",
      "Epoch: 9300, Accuracy: 0.9975312967627021\n",
      "Epoch: 9350, Accuracy: 0.9975514828890826\n",
      "Epoch: 9400, Accuracy: 0.9975713565559603\n",
      "Epoch: 9450, Accuracy: 0.9975909247832021\n",
      "Epoch: 9500, Accuracy: 0.9976101943854493\n",
      "Epoch: 9550, Accuracy: 0.9976291719794695\n",
      "Epoch: 9600, Accuracy: 0.997647863991199\n",
      "Epoch: 9650, Accuracy: 0.9976662766624866\n",
      "Epoch: 9700, Accuracy: 0.9976844160575602\n",
      "Epoch: 9750, Accuracy: 0.9977022880692217\n",
      "Epoch: 9800, Accuracy: 0.9977198984247893\n",
      "Epoch: 9850, Accuracy: 0.9977372526917957\n",
      "Epoch: 9900, Accuracy: 0.9977543562834553\n",
      "Epoch: 9950, Accuracy: 0.9977712144639099\n"
     ]
    }
   ],
   "source": [
    "# Learning process\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for epoch in range(10000):\n",
    "    Y, H = Forward(X, W1, B1, W2, B2)\n",
    "\n",
    "    W2 += learning_rate * diff_W2(H, Z, Y).T\n",
    "    B2 += learning_rate * diff_B2(Z, Y)\n",
    "    W1 += learning_rate * diff_W1(X, H, Z, Y, W2).T\n",
    "    B1 += learning_rate * diff_B1(Z, Y, W2, H)\n",
    "    if not epoch % 50:\n",
    "        Accuracy = 1 - np.mean((Z - Y)**2)\n",
    "        print(f\"Epoch: {epoch}, Accuracy: {Accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.9979180867921666\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "X_Test = np.random.randint(2, size=[50, 2])\n",
    "Z_Test = np.array([X_Test[:, 0] ^ X_Test[:, 1]]).T\n",
    "Y_Test, H = Forward(X_Test, W1, B1, W2, B2)\n",
    "Accuracy = 1 - np.mean((Z_Test - Y_Test)**2)\n",
    "\n",
    "print('Testing Accuracy: ', Accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20dcf72490f89c174da2c842e68573857612f3c1b1722e617d46dde610562cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
